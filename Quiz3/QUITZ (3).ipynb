{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trN3Vibr18eK"
   },
   "source": [
    "#Image Classification Task:\n",
    "• Load the MNIST dataset.\n",
    "• Build a simple convolutional neural network (CNN) using Keras Sequential model.\n",
    "• Train the CNN model on the MNIST dataset.\n",
    "• Evaluate the model's performance on a test set and report accuracy.\n",
    "• Use grid search to optimize hyperparameters such as learning rate, batch size, and\n",
    "optimizer choice.\n",
    "• Use Callback functions to automate training process like “ReduceLROnPlateau” and keep\n",
    "check on validation loss. Also use history object for result visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3LKnWaf19sK",
    "outputId": "2fd4ce0e-dff0-4b69-89e9-7dce885f99a0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape to [samples][width][height][channels]\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
    "\n",
    "# Normalize from 0-255 to 0-1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# One hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0qCM71o1kV1"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaeuVyhr2lAC",
    "outputId": "e572ea29-d8c8-4c75-f336-3135030452ef"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, callbacks=[reduce_lr], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aofQyYK02m14",
    "outputId": "ed583dde-6026-47ca-adc2-d158f5a7f1ab"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Onw_C5Id2p5t",
    "outputId": "141f4ce4-7e6b-4d51-c5a8-ce3e4051b7d8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "def build_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Grid search settings\n",
    "learning_rates = [0.001, 0.0001]\n",
    "batch_sizes = [32, 64]\n",
    "\n",
    "# Grid search\n",
    "best_accuracy = 0\n",
    "best_lr = 0\n",
    "best_batch = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = build_model(learning_rate=lr)\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=batch_size, verbose=0)\n",
    "        accuracy = np.max(history.history['val_accuracy'])\n",
    "        print(f\"LR={lr}, Batch={batch_size}, Val Accuracy={accuracy}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_lr = lr\n",
    "            best_batch = batch_size\n",
    "\n",
    "print(f\"Best parameters: LR={best_lr}, Batch={best_batch}, with accuracy={best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "vxxobSke2tCO",
    "outputId": "cddd7893-c6a0-42e0-8de8-83f7ff7f84e2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8_y9XAB2wni"
   },
   "source": [
    "#Text Classification Task:\n",
    "* Load the IMDb movie reviews dataset.\n",
    "* Preprocess the text data (tokenization, padding).\n",
    "* Build a simple recurrent neural network (RNN) for text classification.\n",
    "* Train the RNN model on the IMDb dataset.\n",
    "* Evaluate the model's performance on a test set and report accuracy.\n",
    "* Use grid search to optimize hyperparameters such as learning rate, batch size, and\n",
    "optimizer choice.\n",
    "* Use Callback functions to automate training process like “ReduceLROnPlateau” and keep\n",
    "check on validation loss. Also use history object for result visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYi4Mdt4243I",
    "outputId": "797e57c2-ceaa-4522-c69b-2c737c3707c3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load dataset\n",
    "vocab_size = 10000  # Consider only the top 10,000 words in the dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Inspect\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Test samples: {len(x_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLg1B4qZ3H1P"
   },
   "outputs": [],
   "source": [
    "max_length = 500  # Maximum length of all sequences\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMWj6wVF3NQt",
    "outputId": "e71d17ef-d472-4110-c635-f94af1306c9b"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "def build_rnn():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),\n",
    "        SimpleRNN(32),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_rnn()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Zikfp6B3P8T",
    "outputId": "40a424c6-e6ad-4cc2-9fca-ce3d8c53f36f"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYB2KH5o3mkM",
    "outputId": "eb78f6f8-74da-485d-c5ae-8d2cf88f8563"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import numpy as np\n",
    "\n",
    "def build_rnn(optimizer='adam', learning_rate=0.001):\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=10000, output_dim=32, input_length=500),\n",
    "        SimpleRNN(32),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "learning_rates = [0.001, 0.0001]\n",
    "batch_sizes = [64, 128]\n",
    "optimizers = ['adam', 'sgd']\n",
    "\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Training with optimizer={optimizer}, learning_rate={lr}, batch_size={batch_size}\")\n",
    "            model = build_rnn(optimizer=optimizer, learning_rate=lr)\n",
    "            model.fit(x_train, y_train, epochs=3, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "            loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "            print(f\"Accuracy: {acc}, Loss: {loss}\")\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = {'optimizer': optimizer, 'learning_rate': lr, 'batch_size': batch_size}\n",
    "\n",
    "print(f\"Best accuracy: {best_acc} with parameters {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bO50A_qy3RpB",
    "outputId": "b3a270c1-acbe-4baf-c5a5-0c6558334b15"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1y7oYH_U3Wnp",
    "outputId": "b74beb59-92e0-42c6-a118-ab24a64f89af"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Fqshcc4c3Xba",
    "outputId": "ebb46260-f5f0-44a1-d218-015202555bbd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkafEgdy8PYh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
